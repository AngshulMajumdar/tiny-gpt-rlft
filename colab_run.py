# -*- coding: utf-8 -*-
"""ScratchLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4b4N6vx3URkvxQkdhDU9Fyszh2ITLx7
"""

# === Block 1: Mount Drive + clean project artifacts + set paths ===
!pip -q install datasets tokenizers tqdm

import os, shutil, time, json, random
from pathlib import Path
import torch

# Repro
seed = 1337
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
if device == "cuda":
    print("GPU:", torch.cuda.get_device_name(0))

# ---- Mount Google Drive (Colab) ----
from google.colab import drive
drive.mount("/content/drive")

# ---- Project root on Drive (SAFE: we only ever delete inside this folder) ----
PROJECT_NAME = "tinystories_wikitext_gpt_from_scratch"
DRIVE_BASE = Path("/content/drive/MyDrive/colab_runs") / PROJECT_NAME

# If you want "start afresh", we delete the whole project folder on Drive.
# This will NOT touch anything else in your Drive.
if DRIVE_BASE.exists():
    print(f"[CLEANUP] Deleting existing project folder:\n  {DRIVE_BASE}")
    shutil.rmtree(DRIVE_BASE)

# Recreate structure
DRIVE_BASE.mkdir(parents=True, exist_ok=True)
(DRIVE_BASE / "tokenizer").mkdir(parents=True, exist_ok=True)
(DRIVE_BASE / "checkpoints").mkdir(parents=True, exist_ok=True)
(DRIVE_BASE / "logs").mkdir(parents=True, exist_ok=True)
(DRIVE_BASE / "data").mkdir(parents=True, exist_ok=True)

print("Drive project root:", DRIVE_BASE)

# ---- Local (ephemeral) workspace for speed; derived artifacts will be copied/saved to Drive ----
LOCAL_BASE = Path("/content/work")
if LOCAL_BASE.exists():
    shutil.rmtree(LOCAL_BASE)
LOCAL_BASE.mkdir(parents=True, exist_ok=True)
(LOCAL_BASE / "data").mkdir(parents=True, exist_ok=True)

print("Local workspace:", LOCAL_BASE)

# ---- Canonical paths used in later blocks ----
PATHS = {
    "drive_root": str(DRIVE_BASE),
    "drive_tokenizer": str(DRIVE_BASE / "tokenizer"),
    "drive_ckpt": str(DRIVE_BASE / "checkpoints"),
    "drive_logs": str(DRIVE_BASE / "logs"),
    "drive_data": str(DRIVE_BASE / "data"),
    "local_root": str(LOCAL_BASE),
    "local_data": str(LOCAL_BASE / "data"),
}
print(json.dumps(PATHS, indent=2))

# Save run metadata (helps resuming / sanity)
meta = {
    "project": PROJECT_NAME,
    "created_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    "seed": seed,
    "device": device,
}
with open(DRIVE_BASE / "run_meta.json", "w") as f:
    json.dump(meta, f, indent=2)

print("Saved:", DRIVE_BASE / "run_meta.json")

# === Block 2: Build mixed text corpus into Drive shards (resume-safe) ===
import os, json, math, time
from pathlib import Path
from tqdm import tqdm
from datasets import load_dataset

# Load PATHS from the previous block (assumes this variable still exists)
drive_data = Path(PATHS["drive_data"])
mix_dir = drive_data / "mix_shards"
mix_dir.mkdir(parents=True, exist_ok=True)

manifest_path = mix_dir / "manifest.json"

# ---- Size caps (edit if you want) ----
# These caps are for raw UTF-8 text written to shards (not tokenized).
CAP_TINYSTORIES_MB = 200   # main
CAP_WIKITEXT_MB    = 50    # diversity
SHARD_MB           = 25    # each shard file size target

cap_ts_bytes = CAP_TINYSTORIES_MB * 1024 * 1024
cap_wt_bytes = CAP_WIKITEXT_MB    * 1024 * 1024
shard_bytes  = SHARD_MB           * 1024 * 1024

# ---- Resume logic ----
# If manifest exists, we will continue appending new shards after existing ones.
if manifest_path.exists():
    manifest = json.loads(manifest_path.read_text())
    print("[RESUME] Found existing manifest with",
          len(manifest.get("shards", [])), "shards.")
else:
    manifest = {
        "created_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "caps_mb": {"tinystories": CAP_TINYSTORIES_MB, "wikitext": CAP_WIKITEXT_MB},
        "shard_mb": SHARD_MB,
        "shards": [],
        "bytes_written": {"tinystories": 0, "wikitext": 0},
        "examples": {"tinystories": 0, "wikitext": 0},
    }

def _next_shard_index():
    return len(manifest["shards"])

def _open_new_shard(src_name: str):
    idx = _next_shard_index()
    shard_path = mix_dir / f"mix_{idx:04d}_{src_name}.txt"
    f = open(shard_path, "w", encoding="utf-8")
    return idx, shard_path, f

def _flush_close(f):
    f.flush()
    os.fsync(f.fileno())
    f.close()

def _write_dataset_text(dataset_iter, src_name: str, cap_bytes: int):
    written = manifest["bytes_written"][src_name]
    ex_count = manifest["examples"][src_name]

    pbar = tqdm(total=cap_bytes, initial=written, unit="B", unit_scale=True,
                desc=f"Writing {src_name}", dynamic_ncols=True)

    shard_idx, shard_path, f = _open_new_shard(src_name)
    shard_written = 0

    for ex in dataset_iter:
        # Extract text field robustly
        if "text" in ex and isinstance(ex["text"], str):
            s = ex["text"]
        elif "story" in ex and isinstance(ex["story"], str):
            s = ex["story"]
        else:
            continue

        # Normalize: ensure separation + keep newlines
        s = s.strip()
        if not s:
            continue
        s = s + "\n\n"

        b = s.encode("utf-8")
        if written + len(b) > cap_bytes:
            # stop once cap reached
            break

        # If shard target reached, close + open next
        if shard_written + len(b) > shard_bytes:
            _flush_close(f)
            manifest["shards"].append({
                "index": shard_idx,
                "path": str(shard_path),
                "source": src_name,
                "bytes": shard_written
            })
            shard_idx, shard_path, f = _open_new_shard(src_name)
            shard_written = 0

        f.write(s)
        written += len(b)
        shard_written += len(b)
        ex_count += 1
        if ex_count % 200 == 0:
            # periodic manifest checkpoint
            manifest["bytes_written"][src_name] = written
            manifest["examples"][src_name] = ex_count
            manifest_path.write_text(json.dumps(manifest, indent=2))

        pbar.update(len(b))

    # close last shard if it has content
    _flush_close(f)
    if shard_written > 0:
        manifest["shards"].append({
            "index": shard_idx,
            "path": str(shard_path),
            "source": src_name,
            "bytes": shard_written
        })

    manifest["bytes_written"][src_name] = written
    manifest["examples"][src_name] = ex_count
    manifest_path.write_text(json.dumps(manifest, indent=2))
    pbar.close()

    print(f"[DONE] {src_name}: wrote {written/1024/1024:.1f} MB, examples ~{ex_count}, shards now {len(manifest['shards'])}")

# ---- TinyStories (stream) ----
# Many setups have these splits; streaming avoids large RAM/disk downloads.
# We'll use train split; it's huge, but we stop at cap_bytes.
ts = load_dataset("roneneldan/TinyStories", split="train", streaming=True)
_write_dataset_text(ts, "tinystories", cap_ts_bytes)

# ---- WikiText (stream) ----
# We'll use wikitext-103-raw-v1 train split; stop at cap.
wt = load_dataset("Salesforce/wikitext", "wikitext-103-raw-v1", split="train", streaming=True)
_write_dataset_text(wt, "wikitext", cap_wt_bytes)

print("\nManifest saved:", manifest_path)
print("Shard dir:", mix_dir)
print("Total shards:", len(manifest["shards"]))

# === Block 3: Train Byte-level BPE tokenizer (BBPE) and save to Drive ===
import json, time
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer

drive_tokenizer = Path(PATHS["drive_tokenizer"])
mix_dir = Path(PATHS["drive_data"]) / "mix_shards"
manifest_path = mix_dir / "manifest.json"

assert manifest_path.exists(), f"Missing manifest: {manifest_path}"
manifest = json.loads(manifest_path.read_text())

# Collect shard paths
shard_paths = [s["path"] for s in manifest["shards"]]
print("Num shards:", len(shard_paths))
print("Example shard:", shard_paths[0])

# ---- Tokenizer config ----
VOCAB_SIZE = 8192        # good default; you can set 4096 if you want smaller
MIN_FREQUENCY = 2
SPECIAL_TOKENS = ["<pad>", "<bos>", "<eos>"]  # keep minimal

# Train tokenizer
tok = ByteLevelBPETokenizer()
tok.train(
    files=shard_paths,
    vocab_size=VOCAB_SIZE,
    min_frequency=MIN_FREQUENCY,
    special_tokens=SPECIAL_TOKENS
)

# Save (ByteLevelBPETokenizer writes vocab.json + merges.txt)
drive_tokenizer.mkdir(parents=True, exist_ok=True)
tok.save_model(str(drive_tokenizer))

# Also save tokenizer.json (full config)
tok_json_path = drive_tokenizer / "tokenizer.json"
tok.save(str(tok_json_path))

# Save meta
meta = {
    "created_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    "vocab_size": VOCAB_SIZE,
    "min_frequency": MIN_FREQUENCY,
    "special_tokens": SPECIAL_TOKENS,
    "trained_on_manifest": str(manifest_path),
}
(drive_tokenizer / "tokenizer_meta.json").write_text(json.dumps(meta, indent=2))

print("\nSaved tokenizer to:", drive_tokenizer)
print("Files:", [p.name for p in drive_tokenizer.iterdir()])

# === Block 4: Tokenize shards -> train/val .bin memmaps on Drive ===
import os, json, time
import numpy as np
from pathlib import Path
from tokenizers import ByteLevelBPETokenizer

drive_root = Path(PATHS["drive_root"])
drive_data = Path(PATHS["drive_data"])
drive_tokenizer = Path(PATHS["drive_tokenizer"])

mix_dir = drive_data / "mix_shards"
manifest_path = mix_dir / "manifest.json"
assert manifest_path.exists(), f"Missing manifest: {manifest_path}"
manifest = json.loads(manifest_path.read_text())
shard_paths = [s["path"] for s in manifest["shards"]]

# Output dir for tokenized dataset
tok_ds_dir = drive_data / "tokenized"
tok_ds_dir.mkdir(parents=True, exist_ok=True)

train_bin = tok_ds_dir / "train.bin"
val_bin   = tok_ds_dir / "val.bin"
meta_path = tok_ds_dir / "dataset_meta.json"

# If you want to rebuild, delete old tokenized files
for p in [train_bin, val_bin, meta_path]:
    if p.exists():
        p.unlink()

# Load tokenizer
tok = ByteLevelBPETokenizer(
    str(drive_tokenizer / "vocab.json"),
    str(drive_tokenizer / "merges.txt")
)

# Special token ids (we added these during training)
specials = ["<pad>", "<bos>", "<eos>"]
special_ids = {s: tok.token_to_id(s) for s in specials}
print("Special IDs:", special_ids)

# ---- Parameters ----
VAL_FRAC = 0.01   # 1% validation (good enough)
# Choose dtype based on vocab size
vocab_size = len(tok.get_vocab())
dtype = np.uint16 if vocab_size <= 65535 else np.uint32
print("Vocab size:", vocab_size, "| dtype:", dtype)

# ---- Pass 1: count total tokens to size memmaps ----
def count_tokens_in_file(path):
    txt = Path(path).read_text(encoding="utf-8")
    return len(tok.encode(txt).ids)

print("\n[PASS 1] Counting tokens...")
total_tokens = 0
for sp in shard_paths:
    total_tokens += count_tokens_in_file(sp)
print("Total tokens:", total_tokens)

val_tokens = int(total_tokens * VAL_FRAC)
train_tokens = total_tokens - val_tokens
print("Train tokens:", train_tokens, "| Val tokens:", val_tokens)

# ---- Create memmaps ----
train_mm = np.memmap(train_bin, dtype=dtype, mode="w+", shape=(train_tokens,))
val_mm   = np.memmap(val_bin,   dtype=dtype, mode="w+", shape=(val_tokens,))

# ---- Pass 2: fill memmaps ----
print("\n[PASS 2] Encoding + writing...")
t0 = time.time()
t_ptr = 0
v_ptr = 0

# Simple split: first (1-VAL_FRAC) tokens -> train, last VAL_FRAC -> val
# We'll fill train until train_tokens then spill into val.
for sp in shard_paths:
    txt = Path(sp).read_text(encoding="utf-8")
    ids = tok.encode(txt).ids
    ids = np.array(ids, dtype=dtype)

    remaining_train = train_tokens - t_ptr
    if remaining_train > 0:
        take = min(remaining_train, len(ids))
        train_mm[t_ptr:t_ptr+take] = ids[:take]
        t_ptr += take
        ids = ids[take:]

    if len(ids) > 0:
        takev = min(val_tokens - v_ptr, len(ids))
        if takev > 0:
            val_mm[v_ptr:v_ptr+takev] = ids[:takev]
            v_ptr += takev

    if t_ptr >= train_tokens and v_ptr >= val_tokens:
        break

train_mm.flush()
val_mm.flush()

elapsed = time.time() - t0
print(f"Done. Wrote train={t_ptr} val={v_ptr} tokens in {elapsed:.1f}s")

# Save metadata
meta = {
    "created_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    "vocab_size": vocab_size,
    "dtype": "uint16" if dtype == np.uint16 else "uint32",
    "total_tokens": int(total_tokens),
    "train_tokens": int(train_tokens),
    "val_tokens": int(val_tokens),
    "val_frac": VAL_FRAC,
    "special_ids": special_ids,
    "source_manifest": str(manifest_path),
}
meta_path.write_text(json.dumps(meta, indent=2))
print("Saved meta:", meta_path)
print("Tokenized dataset dir:", tok_ds_dir)

# === Block 5: GPT (RoPE + SwiGLU) training with Drive checkpoints + auto-resume ===
import os, re, math, json, time
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# ---------------- Paths ----------------
drive_ckpt = Path(PATHS["drive_ckpt"])
drive_logs = Path(PATHS["drive_logs"])
tok_ds_dir = Path(PATHS["drive_data"]) / "tokenized"

train_bin = tok_ds_dir / "train.bin"
val_bin   = tok_ds_dir / "val.bin"
meta_path = tok_ds_dir / "dataset_meta.json"
assert meta_path.exists(), f"Missing {meta_path}"
meta = json.loads(meta_path.read_text())

vocab_size = meta["vocab_size"]
dtype = np.uint16 if meta["dtype"] == "uint16" else np.uint32

train_data = np.memmap(train_bin, dtype=dtype, mode="r")
val_data   = np.memmap(val_bin,   dtype=dtype, mode="r")

print("Train tokens:", len(train_data), "Val tokens:", len(val_data), "Vocab:", vocab_size)

# ---------------- Config ----------------
@dataclass
class CFG:
    # data
    block_size: int = 512

    # model (fits T4 well)
    n_layer: int = 8
    n_head: int = 8
    n_embd: int = 512
    dropout: float = 0.1

    # optim
    lr: float = 3e-4
    weight_decay: float = 0.1
    betas: tuple = (0.9, 0.95)
    grad_clip: float = 1.0

    # training
    batch_size: int = 8              # per step, per device
    grad_accum: int = 4              # effective batch = batch_size * grad_accum
    max_steps: int = 4000            # you can increase later
    warmup_steps: int = 200
    eval_every: int = 200
    eval_iters: int = 50
    save_every: int = 200
    sample_every: int = 200
    sample_tokens: int = 200

    # misc
    seed: int = 1337
    amp: bool = True

cfg = CFG()

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(cfg.seed)
torch.cuda.manual_seed_all(cfg.seed)

# ---------------- Batch sampler ----------------
def get_batch(split: str):
    data = train_data if split == "train" else val_data
    ix = np.random.randint(0, len(data) - cfg.block_size - 1, size=(cfg.batch_size,))
    x = np.stack([data[i:i+cfg.block_size] for i in ix]).astype(np.int64)
    y = np.stack([data[i+1:i+1+cfg.block_size] for i in ix]).astype(np.int64)
    x = torch.from_numpy(x).to(device, non_blocking=True)
    y = torch.from_numpy(y).to(device, non_blocking=True)
    return x, y

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ["train", "val"]:
        losses = torch.zeros(cfg.eval_iters, device=device)
        for k in range(cfg.eval_iters):
            xb, yb = get_batch(split)
            _, loss = model(xb, yb)
            losses[k] = loss
        out[split] = losses.mean().item()
    model.train()
    return out

# ---------------- RoPE helpers ----------------
def build_rope_cache(seq_len, head_dim, device, base=10000.0):
    # returns cos, sin: [seq_len, head_dim]
    theta = 1.0 / (base ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))
    pos = torch.arange(seq_len, device=device).float()
    freqs = torch.einsum("i,j->ij", pos, theta)  # [seq_len, head_dim/2]
    cos = torch.cos(freqs)
    sin = torch.sin(freqs)
    # expand to head_dim with interleaving
    cos = torch.repeat_interleave(cos, 2, dim=-1)
    sin = torch.repeat_interleave(sin, 2, dim=-1)
    return cos, sin

def apply_rope(x, cos, sin):
    # x: [B, nh, T, hs], cos/sin: [T, hs]
    x1 = x[..., ::2]
    x2 = x[..., 1::2]
    cos_half = cos[..., ::2]
    sin_half = sin[..., ::2]
    # rotate
    out1 = x1 * cos_half - x2 * sin_half
    out2 = x1 * sin_half + x2 * cos_half
    out = torch.stack((out1, out2), dim=-1).flatten(-2)
    return out

# ---------------- Model ----------------
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm = x.pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()
        return x * norm * self.weight

class CausalSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        assert cfg.n_embd % cfg.n_head == 0
        self.nh = cfg.n_head
        self.hs = cfg.n_embd // cfg.n_head
        self.qkv = nn.Linear(cfg.n_embd, 3 * cfg.n_embd, bias=False)
        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)
        self.attn_drop = nn.Dropout(cfg.dropout)
        self.resid_drop = nn.Dropout(cfg.dropout)

        # causal mask
        self.register_buffer("mask", torch.tril(torch.ones(cfg.block_size, cfg.block_size)).view(1, 1, cfg.block_size, cfg.block_size))

        # rope cache (built at init for block_size)
        cos, sin = build_rope_cache(cfg.block_size, self.hs, device="cpu")
        self.register_buffer("rope_cos", cos, persistent=False)
        self.register_buffer("rope_sin", sin, persistent=False)

    def forward(self, x):
        B, T, C = x.size()
        qkv = self.qkv(x)  # [B,T,3C]
        q, k, v = qkv.split(C, dim=2)

        q = q.view(B, T, self.nh, self.hs).transpose(1, 2)  # [B,nh,T,hs]
        k = k.view(B, T, self.nh, self.hs).transpose(1, 2)
        v = v.view(B, T, self.nh, self.hs).transpose(1, 2)

        # RoPE (move caches to device lazily)
        cos = self.rope_cos[:T].to(x.device)
        sin = self.rope_sin[:T].to(x.device)
        q = apply_rope(q, cos, sin)
        k = apply_rope(k, cos, sin)

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.hs)  # [B,nh,T,T]
        att = att.masked_fill(self.mask[:, :, :T, :T].to(x.device) == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        y = att @ v  # [B,nh,T,hs]
        y = y.transpose(1, 2).contiguous().view(B, T, C)  # [B,T,C]
        y = self.resid_drop(self.proj(y))
        return y

class SwiGLU(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)
        self.drop = nn.Dropout(cfg.dropout)
    def forward(self, x):
        return self.drop(self.w3(F.silu(self.w1(x)) * self.w2(x)))

class Block(nn.Module):
    def __init__(self):
        super().__init__()
        self.ln1 = RMSNorm(cfg.n_embd)
        self.attn = CausalSelfAttention()
        self.ln2 = RMSNorm(cfg.n_embd)
        self.mlp  = SwiGLU(cfg.n_embd, 4 * cfg.n_embd)
    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, cfg.n_embd)
        self.drop = nn.Dropout(cfg.dropout)
        self.blocks = nn.ModuleList([Block() for _ in range(cfg.n_layer)])
        self.ln_f = RMSNorm(cfg.n_embd)
        self.lm_head = nn.Linear(cfg.n_embd, vocab_size, bias=False)
        self.lm_head.weight = self.tok_emb.weight  # weight tying

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.size()
        x = self.tok_emb(idx)
        x = self.drop(x)
        for blk in self.blocks:
            x = blk(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)  # [B,T,V]
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return logits, loss

@torch.no_grad()
def generate(model, idx, max_new_tokens, temperature=0.9, top_p=0.95):
    model.eval()
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -cfg.block_size:]
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        probs = F.softmax(logits, dim=-1)

        # nucleus sampling
        sorted_probs, sorted_idx = torch.sort(probs, descending=True)
        cum = torch.cumsum(sorted_probs, dim=-1)
        mask = cum > top_p
        mask[..., 0] = False
        sorted_probs = sorted_probs.masked_fill(mask, 0.0)
        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)

        next_token = torch.multinomial(sorted_probs, num_samples=1)
        next_token = sorted_idx.gather(-1, next_token)
        idx = torch.cat((idx, next_token), dim=1)
    model.train()
    return idx

# ---------------- Checkpoint helpers ----------------
def ckpt_name(step): return f"ckpt_step_{step:07d}.pt"

def latest_checkpoint():
    pts = sorted(drive_ckpt.glob("ckpt_step_*.pt"))
    return pts[-1] if pts else None

def save_checkpoint(step, best_val=None):
    state = {
        "step": step,
        "model": model.state_dict(),
        "optim": optim.state_dict(),
        "scaler": scaler.state_dict() if scaler is not None else None,
        "cfg": cfg.__dict__,
        "meta": meta,
        "best_val": best_val,
        "rng_state": torch.get_rng_state(),
        "cuda_rng_state": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
    }
    path = drive_ckpt / ckpt_name(step)
    torch.save(state, path)
    print(f"[SAVE] {path}")

def load_checkpoint(path):
    ckpt = torch.load(path, map_location="cpu")
    model.load_state_dict(ckpt["model"])
    optim.load_state_dict(ckpt["optim"])
    if scaler is not None and ckpt.get("scaler") is not None:
        scaler.load_state_dict(ckpt["scaler"])
    step0 = ckpt["step"]
    best_val = ckpt.get("best_val", None)
    # restore RNG for reproducibility (optional but nice)
    try:
        torch.set_rng_state(ckpt["rng_state"])
        if torch.cuda.is_available() and ckpt.get("cuda_rng_state") is not None:
            torch.cuda.set_rng_state_all(ckpt["cuda_rng_state"])
    except Exception:
        pass
    print(f"[RESUME] Loaded {path} at step {step0}")
    return step0, best_val

# ---------------- Optim + schedule ----------------
model = GPT().to(device)

optim = torch.optim.AdamW(
    model.parameters(),
    lr=cfg.lr,
    betas=cfg.betas,
    weight_decay=cfg.weight_decay
)

scaler = torch.cuda.amp.GradScaler(enabled=(cfg.amp and device=="cuda"))

def get_lr(step):
    # linear warmup then cosine decay to 10% of lr
    if step < cfg.warmup_steps:
        return cfg.lr * step / max(1, cfg.warmup_steps)
    progress = (step - cfg.warmup_steps) / max(1, cfg.max_steps - cfg.warmup_steps)
    progress = min(max(progress, 0.0), 1.0)
    min_lr = cfg.lr * 0.1
    return min_lr + 0.5 * (cfg.lr - min_lr) * (1.0 + math.cos(math.pi * progress))

# ---------------- Auto-resume ----------------
step = 0
best_val = None
ckpt = latest_checkpoint()
if ckpt is not None:
    step, best_val = load_checkpoint(ckpt)

# ---------------- Training loop ----------------
print("Starting training from step", step, "| max_steps", cfg.max_steps)
t0 = time.time()

for it in range(step, cfg.max_steps):
    # set LR
    lr = get_lr(it)
    for pg in optim.param_groups:
        pg["lr"] = lr

    # gradient accumulation
    optim.zero_grad(set_to_none=True)
    for micro in range(cfg.grad_accum):
        xb, yb = get_batch("train")
        with torch.cuda.amp.autocast(enabled=(cfg.amp and device=="cuda")):
            logits, loss = model(xb, yb)
            loss = loss / cfg.grad_accum

        scaler.scale(loss).backward()

    # clip + step
    if cfg.grad_clip is not None:
        scaler.unscale_(optim)
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)

    scaler.step(optim)
    scaler.update()

    # logging
    if (it + 1) % 20 == 0:
        dt = time.time() - t0
        print(f"step {it+1:6d} | lr {lr:.2e} | loss {loss.item()*cfg.grad_accum:.4f} | elapsed {dt/60:.1f} min")

    # eval
    if (it + 1) % cfg.eval_every == 0:
        losses = estimate_loss()
        v = losses["val"]
        print(f"[EVAL] step {it+1}: train {losses['train']:.4f} | val {v:.4f}")
        if best_val is None or v < best_val:
            best_val = v
            save_checkpoint(it+1, best_val=best_val)
            (drive_logs / "best_val.json").write_text(json.dumps({"step": it+1, "best_val": best_val}, indent=2))

    # periodic save (even if not best)
    if (it + 1) % cfg.save_every == 0:
        save_checkpoint(it+1, best_val=best_val)

    # sample
    if (it + 1) % cfg.sample_every == 0:
        # start from <bos> if it exists else token 0
        bos_id = meta["special_ids"].get("<bos>", 1)
        idx = torch.tensor([[bos_id]], dtype=torch.long, device=device)
        out = generate(model, idx, max_new_tokens=cfg.sample_tokens)
        sample_ids = out[0].tolist()
        # Save ids (decode later with tokenizer block)
        sample_path = drive_logs / f"sample_step_{it+1:07d}.json"
        sample_path.write_text(json.dumps({"step": it+1, "token_ids": sample_ids}, indent=2))
        print(f"[SAMPLE] wrote token ids to {sample_path}")

# === Block 7: Better sampling (no <bos> artifact) + repetition penalty + clean printing ===
!pip -q install tokenizers

import json, math
from pathlib import Path
from types import SimpleNamespace

import torch
import torch.nn as nn
import torch.nn.functional as F
from tokenizers import ByteLevelBPETokenizer

# ---- Drive ----
from google.colab import drive
drive.mount("/content/drive")

PROJECT_NAME = "tinystories_wikitext_gpt_from_scratch"
DRIVE_BASE = Path("/content/drive/MyDrive/colab_runs") / PROJECT_NAME
CKPT_DIR = DRIVE_BASE / "checkpoints"
TOK_DIR  = DRIVE_BASE / "tokenizer"
DATA_DIR = DRIVE_BASE / "data" / "tokenized"
LOG_DIR  = DRIVE_BASE / "logs"

def latest_ckpt():
    pts = sorted(CKPT_DIR.glob("ckpt_step_*.pt"))
    return pts[-1] if pts else None

# best if available else latest
best_json = LOG_DIR / "best_val.json"
ckpt_path = None
if best_json.exists():
    best = json.loads(best_json.read_text())
    cand = CKPT_DIR / f"ckpt_step_{int(best['step']):07d}.pt"
    if cand.exists():
        ckpt_path = cand
ckpt_path = ckpt_path or latest_ckpt()
assert ckpt_path is not None, "No checkpoints found."
print("Loading:", ckpt_path)

ckpt = torch.load(ckpt_path, map_location="cpu")
cfg = SimpleNamespace(**ckpt["cfg"])

meta = json.loads((DATA_DIR / "dataset_meta.json").read_text())
vocab_size = int(meta["vocab_size"])
special_ids = meta.get("special_ids", {})
bos_id = special_ids.get("<bos>", 1)
eos_id = special_ids.get("<eos>", 2)

tok = ByteLevelBPETokenizer(str(TOK_DIR / "vocab.json"), str(TOK_DIR / "merges.txt"))

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, "| vocab:", vocab_size)

# ---------------- RoPE helpers ----------------
def build_rope_cache(seq_len, head_dim, device, base=10000.0):
    theta = 1.0 / (base ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))
    pos = torch.arange(seq_len, device=device).float()
    freqs = torch.einsum("i,j->ij", pos, theta)
    cos = torch.cos(freqs)
    sin = torch.sin(freqs)
    cos = torch.repeat_interleave(cos, 2, dim=-1)
    sin = torch.repeat_interleave(sin, 2, dim=-1)
    return cos, sin

def apply_rope(x, cos, sin):
    x1 = x[..., ::2]; x2 = x[..., 1::2]
    cos_half = cos[..., ::2]; sin_half = sin[..., ::2]
    out1 = x1 * cos_half - x2 * sin_half
    out2 = x1 * sin_half + x2 * cos_half
    return torch.stack((out1, out2), dim=-1).flatten(-2)

# ---------------- Model ----------------
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm = x.pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()
        return x * norm * self.weight

class CausalSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        assert cfg.n_embd % cfg.n_head == 0
        self.nh = cfg.n_head
        self.hs = cfg.n_embd // cfg.n_head
        self.qkv = nn.Linear(cfg.n_embd, 3 * cfg.n_embd, bias=False)
        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd, bias=False)
        self.attn_drop = nn.Dropout(cfg.dropout)
        self.resid_drop = nn.Dropout(cfg.dropout)

        self.register_buffer(
            "mask",
            torch.tril(torch.ones(cfg.block_size, cfg.block_size)).view(1, 1, cfg.block_size, cfg.block_size),
            persistent=True
        )
        cos, sin = build_rope_cache(cfg.block_size, self.hs, device="cpu")
        self.register_buffer("rope_cos", cos, persistent=False)
        self.register_buffer("rope_sin", sin, persistent=False)

    def forward(self, x):
        B, T, C = x.size()
        qkv = self.qkv(x)
        q, k, v = qkv.split(C, dim=2)

        q = q.view(B, T, self.nh, self.hs).transpose(1, 2)
        k = k.view(B, T, self.nh, self.hs).transpose(1, 2)
        v = v.view(B, T, self.nh, self.hs).transpose(1, 2)

        cos = self.rope_cos[:T].to(x.device)
        sin = self.rope_sin[:T].to(x.device)
        q = apply_rope(q, cos, sin)
        k = apply_rope(k, cos, sin)

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.hs)
        att = att.masked_fill(self.mask[:, :, :T, :T].to(x.device) == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.resid_drop(self.proj(y))

class SwiGLU(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)
        self.drop = nn.Dropout(cfg.dropout)
    def forward(self, x):
        return self.drop(self.w3(F.silu(self.w1(x)) * self.w2(x)))

class Block(nn.Module):
    def __init__(self):
        super().__init__()
        self.ln1 = RMSNorm(cfg.n_embd)
        self.attn = CausalSelfAttention()
        self.ln2 = RMSNorm(cfg.n_embd)
        self.mlp  = SwiGLU(cfg.n_embd, 4 * cfg.n_embd)
    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, cfg.n_embd)
        self.drop = nn.Dropout(cfg.dropout)
        self.blocks = nn.ModuleList([Block() for _ in range(cfg.n_layer)])
        self.ln_f = RMSNorm(cfg.n_embd)
        self.lm_head = nn.Linear(cfg.n_embd, vocab_size, bias=False)
        self.lm_head.weight = self.tok_emb.weight
    def forward(self, idx):
        x = self.drop(self.tok_emb(idx))
        for blk in self.blocks:
            x = blk(x)
        x = self.ln_f(x)
        return self.lm_head(x)

model = GPT().to(device)
missing, unexpected = model.load_state_dict(ckpt["model"], strict=False)
print("Loaded. Missing:", len(missing), "Unexpected:", len(unexpected))
model.eval()

# ---------------- Better generation ----------------
@torch.no_grad()
def generate_text(
    prompt: str,
    max_new_tokens: int = 250,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.10,   # 1.0 disables; 1.05-1.2 is typical
    stop_on_eos: bool = True
):
    # encode prompt WITHOUT forcing <bos> (avoids printing it)
    prompt_ids = tok.encode(prompt).ids
    idx = torch.tensor([prompt_ids], dtype=torch.long, device=device)

    for _ in range(max_new_tokens):
        idx_cond = idx[:, -cfg.block_size:]
        logits = model(idx_cond)[:, -1, :]

        # repetition penalty: downweight tokens already seen
        if repetition_penalty and repetition_penalty != 1.0:
            seen = torch.unique(idx[0])
            logits[0, seen] = logits[0, seen] / repetition_penalty

        logits = logits / max(1e-6, temperature)
        probs = F.softmax(logits, dim=-1)

        # nucleus sampling
        sorted_probs, sorted_idx = torch.sort(probs, descending=True)
        cum = torch.cumsum(sorted_probs, dim=-1)
        cut = cum > top_p
        cut[..., 0] = False
        sorted_probs = sorted_probs.masked_fill(cut, 0.0)
        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)

        next_token = torch.multinomial(sorted_probs, num_samples=1)
        next_token = sorted_idx.gather(-1, next_token)

        idx = torch.cat([idx, next_token], dim=1)

        if stop_on_eos and int(next_token.item()) == int(eos_id):
            break

    # decode and clean any literal special tokens if present
    text = tok.decode(idx[0].tolist())
    text = text.replace("<bos>", "").replace("<eos>", "").replace("<pad>", "")
    return text.strip()

# ---------------- Show samples ----------------
prompts = [
    "Once upon a time",
    "The little boy said",
    "In a faraway land",
    "Anna and Ben were playing outside.",
    "Lily found a strange box in the garden."
]

for p in prompts:
    print("\n" + "="*80)
    print("PROMPT:", repr(p))
    print("-"*80)
    print(generate_text(p, max_new_tokens=250, temperature=0.7, top_p=0.9, repetition_penalty=1.10))

# === Block 8: RL fine-tuning (KL-regularized REINFORCE) + Drive checkpoint ===
import copy, json, math, time, random
from pathlib import Path
import torch
import torch.nn.functional as F

# ---- Sanity: require Block 7 objects ----
needed = ["model", "cfg", "tok", "device", "CKPT_DIR", "ckpt", "meta"]
missing_needed = [k for k in needed if k not in globals()]
assert not missing_needed, f"Run this AFTER Block 7. Missing: {missing_needed}"

# ---- Make a frozen reference model for KL penalty ----
ref_model = copy.deepcopy(model).to(device)
ref_model.eval()
for p in ref_model.parameters():
    p.requires_grad = False

model.train()

# ---- RL hyperparams (safe defaults) ----
RL_STEPS = 200                 # try 200 first; can increase later
BATCH_PROMPTS = 8              # prompts per RL step
MAX_NEW_TOKENS = 160
TEMPERATURE = 0.8
TOP_P = 0.9
REPETITION_PENALTY = 1.10      # during generation
KL_BETA = 0.02                 # larger = stay closer to base
LR = 5e-6                      # RL should be small LR
GRAD_CLIP = 1.0

optim = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.0)
scaler = torch.cuda.amp.GradScaler(enabled=(device == "cuda"))

# ---- Reward: simple, fast heuristics for TinyStories quality ----
# You can change weights later.
def repetition_penalty_reward(text: str, n=3):
    toks = text.split()
    if len(toks) < n + 5:
        return 0.0
    ngrams = [" ".join(toks[i:i+n]) for i in range(len(toks)-n+1)]
    uniq = len(set(ngrams))
    rep = 1.0 - (uniq / max(1, len(ngrams)))   # 0 good, closer to 1 bad
    return -rep

def quote_balance_reward(text: str):
    # penalize unbalanced quotes
    q = text.count('"')
    return -0.2 if (q % 2 == 1) else 0.0

def ending_reward(text: str):
    t = text.strip()
    if len(t) < 30: return -0.2
    if t.endswith((".", "!", "?")): return 0.2
    return -0.05

def length_reward(text: str):
    # prefer moderately sized outputs (avoid super short / super long rambles)
    L = len(text.split())
    if L < 40: return -0.1
    if 40 <= L <= 120: return 0.2
    if 120 < L <= 180: return 0.05
    return -0.1

def story_style_reward(text: str):
    # light encouragement of TinyStories-like moral closure (optional)
    lower = text.lower()
    bonus = 0.0
    if "learned that" in lower: bonus += 0.15
    if "sorry" in lower: bonus += 0.05
    return bonus

def reward_fn(prompt: str, completion: str):
    # total reward
    r = 0.0
    r += 0.8 * repetition_penalty_reward(completion, n=3)
    r += quote_balance_reward(completion)
    r += ending_reward(completion)
    r += length_reward(completion)
    r += 0.5 * story_style_reward(completion)
    return float(r)

# ---- Prompt set (small + varied). You can expand later. ----
PROMPTS = [
    "Once upon a time",
    "The little boy said",
    "In a faraway land",
    "Anna and Ben were playing outside.",
    "Lily found a strange box in the garden.",
    "The dragon was lonely.",
    "It was a sunny day at the park.",
    "The rabbit wanted to help his friend.",
    "The little girl felt scared.",
    "Mom said,",
]

# ---- Generation helper (returns token ids) ----
@torch.no_grad()
def sample_completion_ids(prompt: str):
    prompt_ids = tok.encode(prompt).ids
    idx = torch.tensor([prompt_ids], dtype=torch.long, device=device)

    model.eval()
    for _ in range(MAX_NEW_TOKENS):
        idx_cond = idx[:, -cfg.block_size:]
        logits = model(idx_cond)[:, -1, :]  # [1,V]

        # repetition penalty (token-level)
        if REPETITION_PENALTY and REPETITION_PENALTY != 1.0:
            seen = torch.unique(idx[0])
            logits[0, seen] = logits[0, seen] / REPETITION_PENALTY

        logits = logits / max(1e-6, TEMPERATURE)
        probs = F.softmax(logits, dim=-1)

        # nucleus sampling
        sorted_probs, sorted_idx = torch.sort(probs, descending=True)
        cum = torch.cumsum(sorted_probs, dim=-1)
        cut = cum > TOP_P
        cut[..., 0] = False
        sorted_probs = sorted_probs.masked_fill(cut, 0.0)
        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)

        next_token = torch.multinomial(sorted_probs, num_samples=1)
        next_token = sorted_idx.gather(-1, next_token)
        idx = torch.cat([idx, next_token], dim=1)

        # optional early stop on <eos>
        if int(next_token.item()) == int(meta["special_ids"].get("<eos>", 2)):
            break

    model.train()
    return idx[0].tolist(), len(prompt_ids)

def decode_clean(ids):
    t = tok.decode(ids)
    return t.replace("<bos>", "").replace("<eos>", "").replace("<pad>", "").strip()

# ---- Logprob helper for generated tokens only ----
def sum_logprobs(model_, full_ids, prompt_len):
    # full_ids: list[int], includes prompt + generated
    x = torch.tensor([full_ids], dtype=torch.long, device=device)
    with torch.cuda.amp.autocast(enabled=(device == "cuda")):
        logits = model_(x[:, :-1])                # predict next token
        logp = F.log_softmax(logits, dim=-1)      # [1, T-1, V]
    # we want logprobs for tokens from prompt_len .. end-1 (generated tokens)
    # token positions in x are 0..T-1; next-token targets are x[:,1:]
    targets = x[:, 1:]                           # [1, T-1]
    # positions corresponding to generated tokens start at (prompt_len-1) in targets
    start = max(0, prompt_len - 1)
    lp = logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)  # [1, T-1]
    return lp[:, start:].sum(dim=1)              # [1]

# ---- Baseline (EMA) for variance reduction ----
baseline = 0.0
baseline_momentum = 0.95

# ---- Save RL checkpoint to Drive ----
def save_rl_ckpt(step, tag="rlft"):
    path = CKPT_DIR / f"{tag}_step_{step:06d}.pt"
    state = {
        "step": step,
        "model": model.state_dict(),
        "cfg": vars(cfg),
        "meta": meta,
        "base_ckpt": str(ckpt_path) if "ckpt_path" in globals() else None,
        "rl": {
            "KL_BETA": KL_BETA, "LR": LR, "TEMPERATURE": TEMPERATURE, "TOP_P": TOP_P,
            "MAX_NEW_TOKENS": MAX_NEW_TOKENS, "BATCH_PROMPTS": BATCH_PROMPTS
        }
    }
    torch.save(state, path)
    print(f"[SAVE] {path}")

print("\n[RLFT] Starting KL-regularized REINFORCE...")
t0 = time.time()

for step in range(1, RL_STEPS + 1):
    batch = random.sample(PROMPTS, k=BATCH_PROMPTS)

    # collect trajectories + rewards
    traj_ids = []
    prompt_lens = []
    rewards = []
    texts = []

    for p in batch:
        ids, plen = sample_completion_ids(p)
        text = decode_clean(ids)
        # reward based on completion only (remove prompt prefix in text for scoring)
        completion_text = text[len(p):].strip() if text.startswith(p) else text
        r = reward_fn(p, completion_text)

        traj_ids.append(ids)
        prompt_lens.append(plen)
        rewards.append(r)
        texts.append((p, completion_text))

    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=device)  # [B]

    # compute policy/ref logprob sums (generated region only)
    logp_sums = []
    logp_ref_sums = []
    for ids, plen in zip(traj_ids, prompt_lens):
        lp = sum_logprobs(model, ids, plen)
        lpr = sum_logprobs(ref_model, ids, plen)
        logp_sums.append(lp)
        logp_ref_sums.append(lpr)

    logp_sums = torch.cat(logp_sums, dim=0)       # [B]
    logp_ref_sums = torch.cat(logp_ref_sums, dim=0)

    # KL term (detach so reward has no gradient dependence)
    kl = (logp_sums - logp_ref_sums).detach()
    eff_reward = rewards_t - KL_BETA * kl

    # baseline + advantage
    batch_mean = eff_reward.mean().item()
    baseline = baseline_momentum * baseline + (1 - baseline_momentum) * batch_mean
    adv = (eff_reward - baseline).detach()

    # REINFORCE loss: maximize eff_reward -> minimize negative
    loss = -(adv * logp_sums).mean()

    optim.zero_grad(set_to_none=True)
    scaler.scale(loss).backward()
    scaler.unscale_(optim)
    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
    scaler.step(optim)
    scaler.update()

    if step % 10 == 0:
        elapsed = (time.time() - t0) / 60
        print(f"step {step:4d}/{RL_STEPS} | loss {loss.item():+.4f} | "
              f"reward {rewards_t.mean().item():+.3f} | eff {eff_reward.mean().item():+.3f} | "
              f"KL {kl.mean().item():+.3f} | {elapsed:.1f} min")

    # periodic save + quick qualitative sample
    if step % 50 == 0:
        save_rl_ckpt(step)
        # show 1 sample
        sp = "Once upon a time"
        ids, plen = sample_completion_ids(sp)
        print("\n[SAMPLE after RL step", step, "]")
        print(decode_clean(ids)[:1500])
        print()

print("\n[RLFT] Done.")
save_rl_ckpt(RL_STEPS)